"""
ë¼ì¦ˆë² ë¦¬íŒŒì´5 ìµœì í™” ë‚™ìƒ ê°ì§€ + ìƒíƒœ ì „ì´ ë¶„ì„ (MQTT Client)
MoveNet Thunder + Rule-based Fall Detection + State Transition + MQTT

ì£¼ìš” ê¸°ëŠ¥:
1. YOLOv8 ì‚¬ëŒ ê²€ì¶œ í•„í„°ë§ ê°•í™”
2. Rule ê¸°ë°˜ ìì„¸ ë¶„ë¥˜ (Standing, Sitting, Walking, Lying Down)
3. ìƒíƒœ ì „ì´ ë¶„ì„ (Standing/Walking â†’ Lying Down = Fall Down)
4. ìœ„í—˜êµ¬ì—­ ëª¨ë‹ˆí„°ë§
5. MQTTë¥¼ í†µí•œ ì„œë²„ ì „ì†¡ (RAW ë°ì´í„° + ALERT)
"""

import os
import cv2
import time
import argparse
import numpy as np
import tensorflow as tf
from ultralytics import YOLO
import paho.mqtt.client as mqtt
import json
from datetime import datetime, timezone
import base64

# ============================================
# MQTT ì„¤ì •
# ============================================
MQTT_BROKER = "10.10.14.73"  # ì„œë²„ IPë¡œ ë³€ê²½í•˜ì„¸ìš”
MQTT_PORT = 1883
TOPIC_BASE = "project/vision"

# ğŸš¨ğŸš¨ AD_USER ì¸ì¦ ì •ë³´ ì¶”ê°€ ğŸš¨ğŸš¨
MQTT_USERNAME = "PE_USER"      # ë“±ë¡ëœ AD ì‚¬ìš©ì ì´ë¦„
MQTT_PASSWORD = "sksk"  # ë“±ë¡ëœ AD ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ (ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½ í•„ìš”)

# ëª¨ë“ˆ ì´ë¦„ ë° í† í”½ ì„¤ì •
FALL_MODULE = "PE"
RAW_TOPIC = TOPIC_BASE + "/" + FALL_MODULE + "/RAW"
ALERT_TOPIC = TOPIC_BASE + "/" + FALL_MODULE + "/ALERT"
FALL_VIDEO_TOPIC = "project/vision/FALL/VIDEO"  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í† í”½

def now_str():
    """ISO 8601 í˜•ì‹ì˜ í˜„ì¬ UTC ì‹œê°ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

# ============================================
# ì„¤ì •
# ============================================
DEBUG_MODE = True  # Trueë¡œ í•˜ë©´ ìƒì„¸ ë¡œê·¸

# ìœ„í—˜êµ¬ì—­ ì„¤ì •
USE_RATIO = False
DANGER_X_MIN = None
DANGER_X_MAX = 200
DANGER_Y_MIN = None
DANGER_Y_MAX = None

DANGER_X_RATIO_MIN = None
DANGER_X_RATIO_MAX = 0.3
DANGER_Y_RATIO_MIN = None
DANGER_Y_RATIO_MAX = None

ZONE_WARNING_TIME = 3
ZONE_ALERT_TIME = 5
DANGER_AREA_COLOR = (0, 0, 255)

# ë‚™ìƒ íŒë‹¨ ì„¤ì •
FALL_CONFIDENCE_THRESHOLD = 0.70
FALL_FRAMES = 3
FALL_TRANSITION_TIME = 1.0

# MQTT RAW ë°ì´í„° ë°œí–‰ ì£¼ê¸° (í”„ë ˆì„)
RAW_PUBLISH_INTERVAL = 15


# ============================================
# MoveNet í¬ì¦ˆ ì¶”ì • ëª¨ë¸
# ============================================

class MoveNetPose:
    """MoveNet Thunder - ë¼ì¦ˆë² ë¦¬íŒŒì´5 ìµœì í™”"""
    
    def __init__(self, model_type='thunder', device='cpu'):
        print(f"Loading MoveNet {model_type}...")
        self.model_type = model_type
        self.input_size = 256 if model_type == 'thunder' else 192
        self.use_tflite = False
        
        # TFLite íŒŒì¼ì„ ìš°ì„  ì‹œë„
        model_path = f'movenet_{model_type}.tflite'
        if os.path.exists(model_path):
            print(f"Found local TFLite model: {model_path}")
            try:
                self.interpreter = tf.lite.Interpreter(model_path=model_path)
                self.interpreter.allocate_tensors()
                self.use_tflite = True
                print(f"[{now_str()}] âœ… Loaded TFLite model successfully!")
                return
            except Exception as e:
                print(f"[{now_str()}] âŒ TFLite loading failed: {e}")
        
        if not self.use_tflite:
            print(f"[{now_str()}] âš ï¸ MoveNet TFLite model not found or failed to load.")
    
    def predict(self, frame, bboxes, scores=None):
        """í”„ë ˆì„ ì•ˆì˜ ì—¬ëŸ¬ ì‚¬ëŒì— ëŒ€í•œ ìì„¸ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."""
        if bboxes is None or len(bboxes) == 0:
            return []

        poses = []
        use_tflite = hasattr(self, 'interpreter')
        if not use_tflite:
            print("âŒ ERROR: TFLite interpreter not loaded!")
            return []

        for i, bbox in enumerate(bboxes):
            x1, y1, x2, y2 = bbox.astype(int)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)
            
            if x2 <= x1 or y2 <= y1:
                continue
                
            crop_img = frame[y1:y2, x1:x2]
            crop_height, crop_width, _ = crop_img.shape
            
            input_image = tf.convert_to_tensor(crop_img, dtype=tf.uint8)
            
            # ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ ë° íŒ¨ë”©
            resized_img = tf.image.resize_with_pad(input_image, self.input_size, self.input_size)
            input_img_tensor = tf.cast(resized_img, dtype=tf.uint8)
            input_batch = tf.expand_dims(input_img_tensor, axis=0)
            
            # TFLite Inference
            input_details = self.interpreter.get_input_details()
            self.interpreter.set_tensor(input_details[0]['index'], input_batch)
            self.interpreter.invoke()
            
            output_details = self.interpreter.get_output_details()
            keypoints_with_scores = np.squeeze(self.interpreter.get_tensor(output_details[0]['index']))
            
            # ì¢Œí‘œ ë³€í™˜ ë¡œì§
            if crop_height > crop_width:
                padd_x = (self.input_size - (crop_width * self.input_size / crop_height)) / 2
                padd_y = 0
                scale_x = (self.input_size - 2 * padd_x) / crop_width
                scale_y = self.input_size / crop_height
            else:
                padd_x = 0
                padd_y = (self.input_size - (crop_height * self.input_size / crop_width)) / 2
                scale_x = self.input_size / crop_width
                scale_y = (self.input_size - 2 * padd_y) / crop_height

            keypoints = np.zeros((17, 3), dtype=np.float32)
            
            norm_y = keypoints_with_scores[:, 0]
            norm_x = keypoints_with_scores[:, 1]
            
            keypoints[:, 0] = ((norm_x * self.input_size) - padd_x) / scale_x + x1
            keypoints[:, 1] = ((norm_y * self.input_size) - padd_y) / scale_y + y1
            keypoints[:, 2] = keypoints_with_scores[:, 2]
            
            proposal_score = float(np.mean(keypoints[:, 2]))
            
            poses.append({
                'keypoints': keypoints,
                'proposal_score': proposal_score,
                'bbox': bbox
            })
        return poses


# ============================================
# Rule ê¸°ë°˜ ìì„¸ ë¶„ë¥˜
# ============================================

def estimate_motion(prev_kp, curr_kp):
    """í‰ê·  í‚¤í¬ì¸íŠ¸ ì´ë™ëŸ‰"""
    if prev_kp is None or len(prev_kp) == 0:
        return 0.0
    
    valid = (prev_kp[:, 2] > 0.2) & (curr_kp[:, 2] > 0.2)
    if np.sum(valid) < 5:
        return 0.0
    
    diffs = np.linalg.norm(curr_kp[valid, :2] - prev_kp[valid, :2], axis=1)
    motion = float(np.mean(diffs))
    
    return motion


def detect_fall_rule_based(keypoints, prev_keypoints=None):
    """ê· í˜•ì¡íŒ ìì„¸ ë¶„ë¥˜ - Standingê³¼ Sitting ëª¨ë‘ ì •í™•í•˜ê²Œ"""
    
    conf = float(np.mean(keypoints[:, 2]))
    valid_kp = keypoints[keypoints[:, 2] > 0.2]
    
    if len(valid_kp) < 5:
        return 'Unknown', conf * 0.3, {}
    
    # 1. ê¸°ë³¸ ë°”ìš´ë”© ë°•ìŠ¤ íŠ¹ì§•
    width = valid_kp[:, 0].max() - valid_kp[:, 0].min()
    height = valid_kp[:, 1].max() - valid_kp[:, 1].min()
    ratio = width / (height + 1e-6)
    
    # 2. ì£¼ìš” ê´€ì ˆ ìœ„ì¹˜ ì¶”ì¶œ
    shoulder_y = [keypoints[i][1] for i in [5, 6] if keypoints[i][2] > 0.2]
    shoulder_x = [keypoints[i][0] for i in [5, 6] if keypoints[i][2] > 0.2]
    
    hip_y = [keypoints[i][1] for i in [11, 12] if keypoints[i][2] > 0.15]
    hip_x = [keypoints[i][0] for i in [11, 12] if keypoints[i][2] > 0.15]
    
    knee_y = [keypoints[i][1] for i in [13, 14] if keypoints[i][2] > 0.15]
    ankle_y = [keypoints[i][1] for i in [15, 16] if keypoints[i][2] > 0.15]
    
    # 3. ê±°ë¦¬ ê³„ì‚°
    sh_dist = 0
    if len(shoulder_y) > 0 and len(hip_y) > 0:
        sh_dist = abs(np.mean(hip_y) - np.mean(shoulder_y))
    
    hk_dist = 0
    if len(hip_y) > 0 and len(knee_y) > 0:
        hk_dist = abs(np.mean(knee_y) - np.mean(hip_y))
    
    ha_dist = 0
    if len(hip_y) > 0 and len(ankle_y) > 0:
        ha_dist = abs(np.mean(ankle_y) - np.mean(hip_y))
    elif len(hip_y) > 0 and len(knee_y) > 0:
        ha_dist = hk_dist * 1.6
    
    shoulder_width = 0
    if len(shoulder_x) >= 2:
        shoulder_width = abs(max(shoulder_x) - min(shoulder_x))
    
    hip_width = 0
    if len(hip_x) >= 2:
        hip_width = abs(max(hip_x) - min(hip_x))
    
    # 4. ëª¨ì…˜
    motion = 0.0
    if prev_keypoints is not None:
        motion = estimate_motion(prev_keypoints, keypoints)
    
    # 5. ë¹„ìœ¨ íŠ¹ì§•
    upper_ratio = sh_dist / (height + 1e-6)
    lower_ratio = ha_dist / (height + 1e-6)
    thigh_ratio = hk_dist / (height + 1e-6)
    
    horizontal_spread = (shoulder_width + hip_width) / 2
    vertical_horizontal_ratio = height / (horizontal_spread + 1e-6)
    
    details = {
        "ratio": f"{ratio:.2f}",
        "height": f"{height:.0f}",
        "sh": f"{sh_dist:.0f}",
        "ha": f"{ha_dist:.0f}",
        "upper_r": f"{upper_ratio:.2f}",
        "lower_r": f"{lower_ratio:.2f}",
        "vh_ratio": f"{vertical_horizontal_ratio:.2f}",
        "motion": f"{motion:.1f}"
    }
    
    # 6. ê· í˜•ì¡íŒ ë¶„ë¥˜ ë¡œì§
    
    # ğŸ”´ 1ìˆœìœ„: ëˆ„ì›ŒìˆìŒ
    if ratio > 1.1 or vertical_horizontal_ratio < 1.5:
        return 'Lying Down', conf * 0.95, details
    
    # ğŸ”´ 2ìˆœìœ„: ëª…í™•í•œ ê±·ê¸°
    if motion > 8 and height > 100 and lower_ratio > 0.35:
        return 'Walking', conf * 0.92, details
    
    # ğŸ”´ 3ìˆœìœ„: ëª…í™•í•œ ì„œìˆìŒ (Lowerê°€ ë§¤ìš° ë†’ìŒ)
    if lower_ratio >= 0.45 and ratio < 0.50:
        return 'Standing', conf * 0.92, details
    
    # ğŸ”´ 4ìˆœìœ„: ëª…í™•í•œ ì•‰ì•„ìˆìŒ (2ê°€ì§€ ì¡°ê±´ ì¤‘ í•˜ë‚˜)
    # ì¼€ì´ìŠ¤ A: í•˜ì²´ê°€ ë§¤ìš° ì§§ê³  + ê°€ë¡œ ë¹„ìœ¨ ë†’ìŒ
    if lower_ratio < 0.25 and ratio > 0.55:
        return 'Sitting', conf * 0.92, details
    
    # ì¼€ì´ìŠ¤ B: í•˜ì²´ê°€ ì§§ê³  + ìƒì²´ ë¹„ì¤‘ ë†’ê³  + VH ë‚®ìŒ
    if lower_ratio < 0.32 and upper_ratio > 0.50 and vertical_horizontal_ratio < 5.0:
        return 'Sitting', conf * 0.90, details
    
    # ğŸ”´ 5ìˆœìœ„: ì¢…í•© ì ìˆ˜ ê¸°ë°˜ (ê· í˜•ì¡íŒ íŒì •)
    sitting_score = 0
    standing_score = 0
    
    # 1. Lower ë¹„ìœ¨ (ê°€ì¥ ì¤‘ìš”)
    if lower_ratio < 0.20:
        sitting_score += 4
    elif lower_ratio < 0.30:
        sitting_score += 3
    elif lower_ratio < 0.38:
        sitting_score += 1
    elif lower_ratio >= 0.42:
        standing_score += 3
    else:
        standing_score += 1
    
    # 2. Ratio (ê°€ë¡œ/ì„¸ë¡œ)
    if ratio > 0.65:
        sitting_score += 3
    elif ratio > 0.55:
        sitting_score += 2
    elif ratio < 0.45:
        standing_score += 2
    
    # 3. Upper ë¹„ìœ¨
    if upper_ratio > 0.60:
        sitting_score += 2
    elif upper_ratio > 0.50:
        sitting_score += 1
    elif upper_ratio < 0.38:
        standing_score += 1
    
    # 4. VH ë¹„ìœ¨
    if vertical_horizontal_ratio > 10.0:
        standing_score += 2
    elif vertical_horizontal_ratio > 6.0:
        standing_score += 1
    elif vertical_horizontal_ratio < 4.0:
        sitting_score += 2
    elif vertical_horizontal_ratio < 5.5:
        sitting_score += 1
    
    # 5. í‚¤
    if height < 100:
        sitting_score += 2
    elif height > 180:
        standing_score += 1
    
    # ìµœì¢… íŒì • (ë§ˆì§„ 1ë¡œ ê· í˜•)
    if sitting_score > standing_score + 1:
        return 'Sitting', conf * 0.75, details
    elif standing_score > sitting_score + 1:
        return 'Standing', conf * 0.75, details
    else:
        # ë™ì ì´ê±°ë‚˜ ì°¨ì´ê°€ 1ì´ë©´ Lower ìš°ì„ 
        if lower_ratio < 0.35:
            return 'Sitting', conf * 0.70, details
        else:
            return 'Standing', conf * 0.70, details


# ============================================
# ìœ„í—˜êµ¬ì—­ í•¨ìˆ˜
# ============================================

def get_location_details(bbox):
    """ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì´ìš©í•´ ì¤‘ì‹¬ x, í•˜ë‹¨ y ì¢Œí‘œë¥¼ ë°˜í™˜"""
    x1, y1, x2, y2 = bbox
    center_x = (x1 + x2) / 2
    bottom_y = y2
    return int(center_x), int(bottom_y)


def is_in_danger_zone(bbox, frame_width, frame_height):
    """ë°”ìš´ë”©ë°•ìŠ¤ê°€ ìœ„í—˜êµ¬ì—­ì— ìˆëŠ”ì§€ í™•ì¸"""
    center_x, bottom_y = get_location_details(bbox)
    
    if USE_RATIO:
        x_min = int(frame_width * DANGER_X_RATIO_MIN) if DANGER_X_RATIO_MIN is not None else 0
        x_max = int(frame_width * DANGER_X_RATIO_MAX) if DANGER_X_RATIO_MAX is not None else frame_width
        y_min = int(frame_height * DANGER_Y_RATIO_MIN) if DANGER_Y_RATIO_MIN is not None else 0
        y_max = int(frame_height * DANGER_Y_RATIO_MAX) if DANGER_Y_RATIO_MAX is not None else frame_height
    else:
        x_min = DANGER_X_MIN if DANGER_X_MIN is not None else 0
        x_max = DANGER_X_MAX if DANGER_X_MAX is not None else frame_width
        y_min = DANGER_Y_MIN if DANGER_Y_MIN is not None else 0
        y_max = DANGER_Y_MAX if DANGER_Y_MAX is not None else frame_height
    
    in_danger_x = True
    if x_min is not None and center_x < x_min:
        in_danger_x = False
    if x_max is not None and center_x > x_max:
        in_danger_x = False
    
    in_danger_y = True
    if y_min is not None and bottom_y < y_min:
        in_danger_y = False
    if y_max is not None and bottom_y > y_max:
        in_danger_y = False
    
    return in_danger_x and in_danger_y


def draw_danger_area(frame, show_gui):
    """ìœ„í—˜êµ¬ì—­ ì‹œê°í™” (GUI ëª¨ë“œì¼ ë•Œë§Œ)"""
    if not show_gui:
        return frame
    
    h, w = frame.shape[:2]
    overlay = frame.copy()
    
    if USE_RATIO:
        x_min = int(w * DANGER_X_RATIO_MIN) if DANGER_X_RATIO_MIN is not None else 0
        x_max = int(w * DANGER_X_RATIO_MAX) if DANGER_X_RATIO_MAX is not None else w
        y_min = int(h * DANGER_Y_RATIO_MIN) if DANGER_Y_RATIO_MIN is not None else 0
        y_max = int(h * DANGER_Y_RATIO_MAX) if DANGER_Y_RATIO_MAX is not None else h
    else:
        x_min = DANGER_X_MIN if DANGER_X_MIN is not None else 0
        x_max = DANGER_X_MAX if DANGER_X_MAX is not None else w
        y_min = DANGER_Y_MIN if DANGER_Y_MIN is not None else 0
        y_max = DANGER_Y_MAX if DANGER_Y_MAX is not None else h
    
    cv2.rectangle(overlay, (x_min, y_min), (x_max, y_max), DANGER_AREA_COLOR, -1)
    cv2.addWeighted(overlay, 0.2, frame, 0.8, 0, frame)
    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), DANGER_AREA_COLOR, 3)
    cv2.putText(frame, "DANGER ZONE", (x_min + 10, y_min + 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, DANGER_AREA_COLOR, 2)
    
    return frame


def draw_zone_warnings(frame, zone_warnings, show_gui):
    """ì—¬ëŸ¬ ìœ„í—˜êµ¬ì—­ ê²½ê³ ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ (GUI ëª¨ë“œì¼ ë•Œë§Œ)"""
    if not show_gui:
        return frame
    
    y_offset = 50
    
    for i, (track_id, elapsed_time, status) in enumerate(zone_warnings):
        if status == 'warning':
            color = (0, 255, 255)
            text = f"WARNING! Worker #{track_id} in danger zone {elapsed_time:.1f}s"
        elif status == 'danger':
            color = (0, 0, 255)
            text = f"DANGER! Worker #{track_id} in danger zone {elapsed_time:.1f}s"
        else:
            continue
        
        y_pos = y_offset + (i * 35)
        cv2.rectangle(frame, (10, y_pos), (650, y_pos + 30), color, -1)
        cv2.putText(frame, text, (15, y_pos + 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    
    return frame


# ============================================
# ê°œì„ ëœ íŠ¸ë˜ì»¤ (ì‚¬ëŒ í•„í„°ë§ ê°•í™”)
# ============================================

class ImprovedTracker:
    """ê°œì„ ëœ IoU ê¸°ë°˜ íŠ¸ë˜ì»¤ with ì‚¬ëŒ ê²€ì¦"""
    
    def __init__(self, max_age=12):
        self.tracks = {}
        self.next_id = 1
        self.max_age = max_age
        self.frame_count = 0
    
    def _is_valid_person(self, keypoints, bbox):
        """ì‚¬ëŒì¸ì§€ ê²€ì¦í•˜ëŠ” ì¶”ê°€ í•„í„°"""
        # 1. í‰ê·  í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„
        avg_conf = np.mean(keypoints[:, 2])
        if avg_conf < 0.25:
            return False
        
        # 2. ì£¼ìš” ê´€ì ˆ(ì–´ê¹¨, ì—‰ë©ì´) ì¡´ì¬ ì—¬ë¶€
        important_kp = [5, 6, 11, 12]
        important_conf = [keypoints[i][2] for i in important_kp if i < len(keypoints)]
        if len(important_conf) < 2 or np.mean(important_conf) < 0.3:
            return False
        
        # 3. bbox í¬ê¸° ì²´í¬
        x1, y1, x2, y2 = bbox
        width = x2 - x1
        height = y2 - y1
        
        if width < 20 or height < 30:
            return False
        if width > 700 or height > 800:
            return False
        
        # 4. ì¢…íš¡ë¹„ ì²´í¬
        aspect = height / (width + 1e-6)
        if aspect < 0.4:
            return False
        
        return True
    
    def update(self, detections):
        """detections: List of {'bbox': [x1,y1,x2,y2], 'keypoints': array, 'score': float}"""
        self.frame_count += 1
        
        # ê¸°ì¡´ íŠ¸ë™ ë‚˜ì´ ì¦ê°€
        for track_id in list(self.tracks.keys()):
            self.tracks[track_id]['age'] += 1
            if self.tracks[track_id]['age'] > self.max_age:
                if DEBUG_MODE:
                    print(f"[TRACKER] Removing track #{track_id} (age: {self.tracks[track_id]['age']})")
                del self.tracks[track_id]
        
        # ì‚¬ëŒ ê²€ì¦ í•„í„°ë§ ì¶”ê°€
        valid_detections = []
        for det in detections:
            if self._is_valid_person(det['keypoints'], det['bbox']):
                valid_detections.append(det)
            elif DEBUG_MODE:
                print(f"  Filtered out invalid detection (low quality)")
        
        # IoU ë§¤ì¹­
        for det in valid_detections:
            best_iou = 0
            best_id = None
            
            for track_id, track in self.tracks.items():
                iou = self._calculate_iou(det['bbox'], track['bbox'])
                if iou > best_iou and iou > 0.3:
                    best_iou = iou
                    best_id = track_id
            
            if best_id is not None:
                # ê¸°ì¡´ íŠ¸ë™ ì—…ë°ì´íŠ¸
                self.tracks[best_id]['bbox'] = det['bbox']
                self.tracks[best_id]['keypoints'].append(det['keypoints'])
                self.tracks[best_id]['age'] = 0
            else:
                # ìƒˆ íŠ¸ë™ ìƒì„±
                self.tracks[self.next_id] = {
                    'bbox': det['bbox'],
                    'keypoints': [det['keypoints']],
                    'age': 0
                }
                self.next_id += 1
        
        return list(self.tracks.keys())
    
    def _calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 < x1 or y2 < y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / (union + 1e-6)


# ============================================
# YOLOv8 ê²€ì¶œê¸°
# ============================================

class ImprovedYOLODetector:
    """ê°•í™”ëœ ì‚¬ëŒ ê²€ì¶œ í•„í„°ë§"""
    
    def __init__(self, model_name='yolov8n.pt', conf_thres=0.5, device='cpu'):
        self.model = YOLO(model_name)
        self.conf_thres = conf_thres
        self.device = device
    
    def detect(self, frame):
        """ì‚¬ëŒë§Œ ê²€ì¶œ (ì¶”ê°€ í•„í„°ë§)"""
        results = self.model.predict(
            frame,
            conf=self.conf_thres,
            classes=[0],
            device=self.device,
            verbose=False,
            imgsz=320
        )
        
        if len(results) == 0 or len(results[0].boxes) == 0:
            return [], []
        
        boxes = results[0].boxes
        bboxes = []
        scores = []
        
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0].cpu().numpy())
            cls = int(box.cls[0].cpu().numpy())
            
            if cls != 0:
                continue
            
            width = x2 - x1
            height = y2 - y1
            
            if width < 40 or height < 35:
                if DEBUG_MODE:
                    print(f"  Filtered bbox: too small ({width:.0f}x{height:.0f})")
                continue
            
            if width > frame.shape[1] * 0.9 or height > frame.shape[0] * 0.9:
                if DEBUG_MODE:
                    print(f"  Filtered bbox: too large ({width:.0f}x{height:.0f})")
                continue
            
            aspect = height / (width + 1e-6)
            if aspect < 0.3:
                if DEBUG_MODE:
                    print(f"  Filtered bbox: invalid aspect ratio {aspect:.2f}")
                continue
            
            bboxes.append([x1, y1, x2, y2])
            scores.append(conf)
        
        return np.array(bboxes, dtype=np.float32), np.array(scores, dtype=np.float32)


# ============================================
# MQTT ë°œí–‰ í•¨ìˆ˜
# ============================================
def on_connect(client, userdata, flags, rc):
    """MQTT ì—°ê²° ì½œë°±"""
    if rc == 0:
        print(f"[{now_str()}] âœ… MQTT Connected successfully.")
    else:
        print(f"[{now_str()}] âŒ MQTT Connection failed with code {rc}")

def publish_mqtt_message(client, topic, payload):
    """JSON ë©”ì‹œì§€ë¥¼ MQTTë¡œ ë°œí–‰"""
    try:
        json_payload = json.dumps(payload, ensure_ascii=False)
        client.publish(topic, json_payload, qos=0)
        if DEBUG_MODE:
            msg_type = "ALERT" if "ALERT" in topic else "RAW"
            level = payload.get("level", "N/A")
            print(f"[{now_str()}] [MQTT SEND - {msg_type}:{level}] {topic}")
    except Exception as e:
        print(f"[{now_str()}] [MQTT ERROR] Failed to publish to {topic}: {e}")


# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================

def main():
    parser = argparse.ArgumentParser(description='Improved Fall Detection with MQTT Client')
    parser.add_argument('--camera', type=str, default='0', help='Camera source')
    parser.add_argument('--device', type=str, default='cpu', help='cpu only for RPi5')
    parser.add_argument('--model', type=str, default='thunder', choices=['thunder', 'lightning'])
    parser.add_argument('--show-gui', action='store_true', help='Show GUI window with visualizations')
    args = parser.parse_args()
    
    print("="*60)
    print("Improved Fall Detection System (MQTT Client)")
    print("- Object Detection: YOLOv8n (Enhanced Filtering)")
    print("- Pose: MoveNet " + args.model.title())
    print("- Device: CPU")
    print(f"- MQTT Broker: {MQTT_BROKER}:{MQTT_PORT} / Module: {FALL_MODULE}")
    print(f"- GUI Mode: {'Enabled' if args.show_gui else 'Disabled'}")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\n1ï¸âƒ£ Loading models...")
    detector = ImprovedYOLODetector(model_name='yolov8n.pt', conf_thres=0.5, device='cpu')
    pose_model = MoveNetPose(model_type=args.model)
    
    # íŠ¸ë˜ì»¤
    tracker = ImprovedTracker(max_age=12)
    
    # MQTT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # 3. MQTT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    mqtt_client = mqtt.Client(client_id="PE_Client", protocol=mqtt.MQTTv311)
    
    # ì‚¬ìš©ì ì´ë¦„ ë° ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
    mqtt_client.username_pw_set(MQTT_USERNAME, MQTT_PASSWORD)

    mqtt_client.on_connect = on_connect
    try:
        mqtt_client.connect(MQTT_BROKER, MQTT_PORT, 60)
        mqtt_client.loop_start()
    except Exception as e:
        print(f"[{now_str()}] âŒ Failed to connect to MQTT broker: {e}")
    
    # ì¹´ë©”ë¼
    print("\n2ï¸âƒ£ Opening camera...")
    cam_source = args.camera
    if cam_source.isdigit():
        cap = cv2.VideoCapture(int(cam_source))
    else:
        cap = cv2.VideoCapture(cam_source)
    
    if not cap.isOpened():
        print(f"[{now_str()}] âŒ Cannot open camera!")
        mqtt_client.loop_stop()
        return
    
    print(f"[{now_str()}] âœ… Camera opened")
    
    # ìƒíƒœ ë³€ìˆ˜
    fall_counters = {}
    fall_alerted = {}
    zone_timers = {}
    previous_states = {}
    
    # Alert ì „ì†¡ ìƒíƒœ (ì¤‘ë³µ ì „ì†¡ ë°©ì§€)
    alert_sent_zone = {}
    
    fps_time = time.time()
    frame_count = 0
    
    # YOLO í”„ë ˆì„ ìŠ¤í‚µ ë³€ìˆ˜
    last_bboxes = []
    last_scores = []
    yolo_skip_counter = 0
    YOLO_SKIP_FRAMES = 3
    
    quit_msg = "Press 'q' to quit" if args.show_gui else "Press Ctrl+C to quit"
    print(f"[{now_str()}] 3ï¸âƒ£ Starting detection... ({quit_msg})\n")
    
    while True:
        try:
            ret, frame = cap.read()
            if not ret:
                print(f"[{now_str()}] End of stream or video.")
                break
            
            frame_count += 1
            h, w = frame.shape[:2]
            current_time = time.time()
            
            # ìœ„í—˜êµ¬ì—­ ê·¸ë¦¬ê¸° (GUI ëª¨ë“œì¼ ë•Œë§Œ)
            frame = draw_danger_area(frame, args.show_gui)
            
            # ì‚¬ëŒ ê²€ì¶œ (í”„ë ˆì„ ìŠ¤í‚µ ì ìš©)
            yolo_skip_counter += 1
            if yolo_skip_counter >= YOLO_SKIP_FRAMES:
                bboxes, scores = detector.detect(frame)
                last_bboxes = bboxes
                last_scores = scores
                yolo_skip_counter = 0
            else:
                bboxes = last_bboxes
                scores = last_scores
            
            if DEBUG_MODE and frame_count % 30 == 0:
                print(f"\nFrame {frame_count}: Detected {len(bboxes)} persons")
            
            # í¬ì¦ˆ ì¶”ì •
            detections = []
            if len(bboxes) > 0:
                poses = pose_model.predict(frame, bboxes, scores)
                
                for pose, bbox in zip(poses, bboxes):
                    keypoints = pose['keypoints']
                    
                    detections.append({
                        'bbox': bbox,
                        'keypoints': keypoints,
                        'score': pose['proposal_score']
                    })
            
            # íŠ¸ë˜í‚¹
            current_tracks = tracker.update(detections)
            
            # RAW ë°ì´í„°ìš© ë¦¬ìŠ¤íŠ¸ ë° ìœ„í—˜êµ¬ì—­ ê²½ê³  ë¦¬ìŠ¤íŠ¸
            raw_detections_list = []
            zone_warnings = []
            is_person_detected = len(current_tracks) > 0
            
            # ê° íŠ¸ë™ ì²˜ë¦¬
            for track_id in current_tracks:
                track = tracker.tracks[track_id]
                bbox = track['bbox']
                keypoints_list = track['keypoints']
                if len(keypoints_list) < 1:
                    continue
                
                x1, y1, x2, y2 = bbox.astype(int)
                center_x, bottom_y = get_location_details(bbox)
                
                # --- [A] ìœ„í—˜êµ¬ì—­ ì²´í¬ (ZONE) ---
                in_zone = is_in_danger_zone(bbox, w, h)
                
                if in_zone:
                    if track_id not in zone_timers:
                        zone_timers[track_id] = current_time
                        alert_sent_zone[track_id] = False
                        if DEBUG_MODE:
                            print(f"[DANGER ZONE] Worker #{track_id} entered!")
                    
                    elapsed = current_time - zone_timers[track_id]
                    
                    # ğŸš¨ 1. ZONE CRITICAL ALERT (ì„ê³„ì¹˜ ì´ˆê³¼)
                    if elapsed >= ZONE_ALERT_TIME and not alert_sent_zone.get(track_id, False):
                        # ALERT ë©”ì‹œì§€ ë°œí–‰
                        alert_payload = {
                            "timestamp": now_str(),
                            "module": FALL_MODULE,
                            "level": "CRITICAL",
                            "message": f"ğŸš¨ DANGER ZONE CRITICAL: Worker #{track_id} in high-risk area for {elapsed:.1f}s. Immediate removal required.",
                            "details": [{"track_id": track_id, "action": "InDangerZoneCritical", "location": f"({center_x}, {bottom_y})"}]
                        }
                        publish_mqtt_message(mqtt_client, ALERT_TOPIC, alert_payload)
                        alert_sent_zone[track_id] = True
                        print(f"[{now_str()}] ğŸš¨ğŸš¨ [CRITICAL ALERT SENT] Zone alert for Worker #{track_id} ({elapsed:.1f}s)")

                    # âš ï¸ 2. ZONE WARNING (ê²½ê³  ì„ê³„ì¹˜ ì ‘ê·¼)
                    elif elapsed >= ZONE_WARNING_TIME and not alert_sent_zone.get(track_id, False): 
                        if DEBUG_MODE:
                            print(f"[{now_str()}] âš ï¸ [WARNING] Worker #{track_id} in zone for {elapsed:.1f}s.")
                        
                else:
                    if track_id in zone_timers:
                        final_time = current_time - zone_timers[track_id]
                        if DEBUG_MODE:
                            print(f"[DANGER ZONE] Worker #{track_id} left (stayed {final_time:.1f}s)")
                        del zone_timers[track_id]
                        alert_sent_zone[track_id] = False
                
                # --- [B] ë‚™ìƒ ê°ì§€ (FALL) ---
                # ğŸ”´ Rule ê¸°ë°˜ ìì„¸ ë¶„ë¥˜
                current_kp = keypoints_list[-1]
                prev_kp = keypoints_list[-2] if len(keypoints_list) >= 2 else None
                
                rule_action, rule_conf, details = detect_fall_rule_based(current_kp, prev_kp)
                
                # ğŸ”´ ìƒíƒœ ì „ì´ ë¶„ì„: Fall Down ê°ì§€
                # ì´ì „ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                if track_id in previous_states:
                    prev_action = previous_states[track_id]['action']
                    state_start_time = previous_states[track_id]['state_start_time']
                    time_since_state_start = current_time - state_start_time
                else:
                    prev_action = None
                    state_start_time = current_time
                    time_since_state_start = 0
                
                # ë‚™ìƒ ì „ì´ ê°ì§€: (Standing/Walking) â†’ Lying Down
                final_action = rule_action
                final_conf = rule_conf
                new_state_start_time = state_start_time
                
                # ğŸ”´ ì¼€ì´ìŠ¤ 1: Lying Downì€ ë¬´ì¡°ê±´ Fall Downìœ¼ë¡œ ë³€ê²½!
                if rule_action == 'Lying Down':
                    final_action = 'Fall Down'
                    final_conf = rule_conf * 1.15
                    
                    # ì²˜ìŒ ëˆ„ìš´ ê²ƒì´ë©´ ìƒˆë¡œìš´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                    if prev_action != 'Fall Down':
                        new_state_start_time = current_time
                        if DEBUG_MODE:
                            print(f"\nğŸš¨ [FALL DETECTED!] Worker #{track_id}")
                            print(f"   Transition: {prev_action} â†’ Fall Down")
                    # ì´ë¯¸ Fall Down ìƒíƒœë©´ ì‹œê°„ ìœ ì§€
                    else:
                        if DEBUG_MODE and frame_count % 30 == 0:
                            print(f"[FALL SUSTAINED] Worker #{track_id} in Fall Down ({time_since_state_start:.1f}s)")
                
                # ğŸ”´ ì¼€ì´ìŠ¤ 2: Fall Down â†’ Standing/Walking/Sitting (íšŒë³µ)
                elif prev_action == 'Fall Down' and rule_action in ['Standing', 'Walking', 'Sitting']:
                    final_action = rule_action
                    new_state_start_time = current_time
                    if DEBUG_MODE:
                        print(f"[RECOVERY] Worker #{track_id} recovered from Fall Down â†’ {rule_action}")
                
                # ğŸ”´ ì¼€ì´ìŠ¤ 3: ìƒíƒœ ë³€ê²½
                elif prev_action != rule_action:
                    final_action = rule_action
                    new_state_start_time = current_time
                
                # ğŸ”´ ì¼€ì´ìŠ¤ 4: ìƒíƒœ ìœ ì§€
                else:
                    final_action = rule_action
                
                # ğŸ”´ í˜„ì¬ ìƒíƒœ ì €ì¥
                previous_states[track_id] = {
                    'action': final_action,
                    'state_start_time': new_state_start_time
                }
                
                # ë””ë²„ê·¸ ì¶œë ¥
                if DEBUG_MODE and frame_count % 10 == 0:
                    print(f"[DEBUG] ID:{track_id} | Final:'{final_action}'({final_conf:.2f}) | "
                          f"Prev:'{prev_action}' | "
                          f"Rule:'{rule_action}' | "
                          f"H:{details.get('height', 'N/A')} | "
                          f"R:{details.get('ratio', 'N/A')} | "
                          f"Upper:{details.get('upper_r', 'N/A')} | "
                          f"Lower:{details.get('lower_r', 'N/A')} | "
                          f"VH:{details.get('vh_ratio', 'N/A')} | "
                          f"M:{details.get('motion', 'N/A')}")
                
                # ë‚™ìƒ ì¹´ìš´í„°
                if track_id not in fall_counters:
                    fall_counters[track_id] = 0
                if track_id not in fall_alerted:
                    fall_alerted[track_id] = False
                
                if final_action in ['Fall Down'] and final_conf >= FALL_CONFIDENCE_THRESHOLD:
                    fall_counters[track_id] += 1
                else:
                    fall_counters[track_id] = 0
                    fall_alerted[track_id] = False
                
                # ğŸš¨ 3. FALL CRITICAL ALERT (ë‚™ìƒ í™•ì •)
                if fall_counters[track_id] >= FALL_FRAMES and not fall_alerted.get(track_id, False):
                    alert_payload = {
                        "timestamp": now_str(),
                        "module": FALL_MODULE,
                        "level": "CRITICAL",
                        "message": f"ğŸš¨ CRITICAL FALL DETECTED: Worker #{track_id} is {final_action.lower()} at location ({center_x}, {bottom_y}). Immediate assistance required.",
                        "details": [{"track_id": track_id, "action": final_action, "confidence": float(final_conf), "location": f"({center_x}, {bottom_y})"}]
                    }
                    publish_mqtt_message(mqtt_client, ALERT_TOPIC, alert_payload)
                    fall_alerted[track_id] = True
                    print(f"[{now_str()}] ğŸš¨ğŸš¨ [CRITICAL ALERT SENT] Fall alert for Worker #{track_id}")
                
                # 4. RAW ë°ì´í„° ê¸°ë¡
                raw_detections_list.append({
                    "track_id": int(track_id),
                    "object_type": "Person",
                    "action": final_action,
                    "confidence": float(final_conf),
                    "x_center": center_x,
                    "y_bottom": bottom_y,
                    "in_danger_zone": in_zone
                })
            
            # GUI ëª¨ë“œ: í™”ë©´ì— ì‹œê°í™”
            if args.show_gui:
                # ê° íŠ¸ë™ì— ëŒ€í•œ ì‹œê°í™”
                for track_id in current_tracks:
                    track = tracker.tracks[track_id]
                    bbox = track['bbox']
                    keypoints_list = track['keypoints']
                    
                    if len(keypoints_list) < 1:
                        continue
                    
                    x1, y1, x2, y2 = bbox.astype(int)
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    
                    # íŠ¸ë™ IDë³„ ìƒíƒœ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    if track_id in previous_states:
                        final_action = previous_states[track_id]['action']
                    else:
                        final_action = 'Unknown'
                    
                    # ìƒ‰ìƒ ê²°ì •
                    if fall_counters.get(track_id, 0) >= FALL_FRAMES:
                        clr = (0, 0, 255)  # ğŸ”´ í™•ì • ë‚™ìƒ
                    elif final_action == 'Fall Down':
                        clr = (0, 0, 255)  # ğŸ”´ Fall Down
                    elif final_action == 'Standing':
                        clr = (0, 255, 0)  # ğŸŸ¢ ì •ìƒ
                    elif final_action == 'Sitting':
                        clr = (0, 255, 255)  # ğŸŸ¡ ì•‰ìŒ
                    elif final_action == 'Walking':
                        clr = (255, 255, 0)  # í•˜ëŠ˜ìƒ‰
                    else:
                        clr = (255, 255, 255)  # âšª Unknown
                    
                    # Bounding box ê·¸ë¦¬ê¸°
                    cv2.rectangle(frame, (x1, y1), (x2, y2), clr, 2)
                    
                    # Track ID
                    cv2.putText(frame, f'ID:{track_id}', center,
                               cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 0, 0), 2)
                    
                    # Action ì •ë³´
                    action_text = f'{final_action}'
                    cv2.putText(frame, action_text, (x1 + 5, y1 + 20),
                               cv2.FONT_HERSHEY_COMPLEX, 0.5, clr, 2)
                    
                    # ìœ„í—˜êµ¬ì—­ ì²´í¬ í‘œì‹œ
                    in_zone = is_in_danger_zone(bbox, w, h)
                    if in_zone and track_id in zone_timers:
                        elapsed = current_time - zone_timers[track_id]
                        time_text = f"Zone: {elapsed:.1f}s"
                        cv2.putText(frame, time_text, (x1, y1 - 10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                
                # ìœ„í—˜êµ¬ì—­ ê²½ê³  í‘œì‹œ
                zone_warnings_display = []
                for track_id in current_tracks:
                    if track_id in zone_timers:
                        elapsed = current_time - zone_timers[track_id]
                        if elapsed >= ZONE_ALERT_TIME:
                            zone_warnings_display.append((track_id, elapsed, 'danger'))
                        elif elapsed >= ZONE_WARNING_TIME:
                            zone_warnings_display.append((track_id, elapsed, 'warning'))
                
                if zone_warnings_display:
                    frame = draw_zone_warnings(frame, zone_warnings_display, args.show_gui)
                
                # FPS ë° ì •ë³´ í‘œì‹œ
                fps = 1.0 / (time.time() - fps_time + 1e-6)
                info_text = f'Frame: {frame_count} | FPS: {fps:.1f} | Persons: {len(current_tracks)}'
                cv2.putText(frame, info_text,
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('Fall Detection System', frame)
                
                # 'q' í‚¤ë¡œ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print(f"\n[{now_str()}] [INFO System] Program stopped by user (pressed 'q').")
                    break
            
            # 5. ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì „ì†¡
            try:
                # í”„ë ˆì„ ì••ì¶• (JPEG) ë° Base64 ì¸ì½”ë”©
                ret_enc, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 50])
                
                if ret_enc:
                    jpg_as_text = base64.b64encode(buffer.tobytes())
                    
                    # VIDEO í† í”½ìœ¼ë¡œ ë°œí–‰ (QoS=0)
                    mqtt_client.publish(FALL_VIDEO_TOPIC, jpg_as_text, qos=0)
                    
                    if DEBUG_MODE and frame_count % 30 == 0:
                        print(f"[{now_str()}] [PUB-FALL-VIDEO] âœ… Base64 frame sent to {FALL_VIDEO_TOPIC} (Size: {len(jpg_as_text)/1024:.1f} KB)")
                
            except Exception as e:
                print(f"[{now_str()}] [ERROR] âŒ FALL Video streaming publish failed: {e}")
            
            # 6. RAW ë°ì´í„° ë°œí–‰ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if frame_count % RAW_PUBLISH_INTERVAL == 0 and is_person_detected:
                raw_payload = {
                    "timestamp": now_str(),
                    "module": FALL_MODULE,
                    "level": "INFO",
                    "detections": raw_detections_list,
                    "person_detected": is_person_detected
                }
                publish_mqtt_message(mqtt_client, RAW_TOPIC, raw_payload)
                
                # FPS ê³„ì‚° ë° ì¶œë ¥
                end_time = time.time()
                fps = 1.0 / (end_time - fps_time + 1e-6)
                fps_time = end_time
                print(f"[{now_str()}] [PUB-FALL-RAW:INFO] âœ… RAW data sent (Tracks: {len(current_tracks)}) (FPS: {fps:.1f})")
            
            # CPU ì‚¬ìš©ëŸ‰ì„ ë‚®ì¶”ê¸° ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.01)

        except KeyboardInterrupt:
            print(f"\n[{now_str()}] [INFO System] Measurement stopped by user (Ctrl+C).")
            break
        except Exception as e:
            print(f"\n[{now_str()}] [ERROR System] An unexpected error occurred: {e}")
            break
    
    # ì •ë¦¬
    if cap.isOpened():
        cap.release()
    
    if args.show_gui:
        cv2.destroyAllWindows()
    
    print("\n" + "="*60)
    print("Program terminated. Closing MQTT connection.")
    mqtt_client.loop_stop()
    mqtt_client.disconnect()
    print("="*60)


if __name__ == '__main__':
    # TensorFlow ë¡œê·¸ë¥¼ ì–µì œí•˜ì—¬ í„°ë¯¸ë„ ì¶œë ¥ì„ ê¹”ë”í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    main()
