"""
ë¼ì¦ˆë² ë¦¬íŒŒì´5 ìµœì í™” ë‚™ìƒ ê°ì§€ + ìƒíƒœ ì „ì´ ë¶„ì„ (MQTT Client)
MoveNet Thunder + Rule-based Fall Detection + State Transition + MQTT

*** ì£¼ìš” ìˆ˜ì • ì‚¬í•­ (2025-10-17) ***
1. [CRITICAL FIX] MoveNetPose: ë¶€ì •í™•í•œ í¬ë¡­ ë° ì¢Œí‘œ ë³€í™˜ ë¡œì§ì„ ë‹¨ìˆœí•˜ê³  ì •í™•í•œ ë°©ì‹ìœ¼ë¡œ ì „ë©´ êµì²´.
2. [CRITICAL FIX] main-loop: YOLO í”„ë ˆì„ ìŠ¤í‚µ ì‹œ ë°œìƒí•˜ë˜ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸ ë²„ê·¸ ìˆ˜ì •.
3. [IMPROVEMENT] YOLO: imgszë¥¼ 416ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì†ë„ì™€ ì •í™•ë„ ê· í˜• ê°œì„ .
4. [IMPROVEMENT] GUI: í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ê¸°ëŠ¥ ì¶”ê°€í•˜ì—¬ ë””ë²„ê¹… í¸ì˜ì„± ì¦ëŒ€.
"""

import os
import cv2
import time
import argparse
import numpy as np
import tensorflow as tf
from ultralytics import YOLO
import paho.mqtt.client as mqtt
import json
from datetime import datetime, timezone
import base64

# ============================================
# MQTT ì„¤ì •
# ============================================
MQTT_BROKER = "10.10.14.73"  # ì„œë²„ IPë¡œ ë³€ê²½í•˜ì„¸ìš”
MQTT_PORT = 1883
TOPIC_BASE = "project/vision"

# ğŸš¨ğŸš¨ AD_USER ì¸ì¦ ì •ë³´ ì¶”ê°€ ğŸš¨ğŸš¨
MQTT_USERNAME = "PE_USER"      # ë“±ë¡ëœ AD ì‚¬ìš©ì ì´ë¦„
MQTT_PASSWORD = "sksk"  # ë“±ë¡ëœ AD ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ (ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½ í•„ìš”)

# ëª¨ë“ˆ ì´ë¦„ ë° í† í”½ ì„¤ì •
FALL_MODULE = "PE"
RAW_TOPIC = TOPIC_BASE + "/" + FALL_MODULE + "/RAW"
ALERT_TOPIC = TOPIC_BASE + "/" + FALL_MODULE + "/ALERT"
FALL_VIDEO_TOPIC = "project/vision/FALL/VIDEO"  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í† í”½

def now_str():
    """ISO 8601 í˜•ì‹ì˜ í˜„ì¬ UTC ì‹œê°ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

# ============================================
# ì„¤ì •
# ============================================
DEBUG_MODE = True  # Trueë¡œ í•˜ë©´ ìƒì„¸ ë¡œê·¸

# ìœ„í—˜êµ¬ì—­ ì„¤ì •
USE_RATIO = False
DANGER_X_MIN = None
DANGER_X_MAX = 200
DANGER_Y_MIN = None
DANGER_Y_MAX = None

DANGER_X_RATIO_MIN = None
DANGER_X_RATIO_MAX = 0.3
DANGER_Y_RATIO_MIN = None
DANGER_Y_RATIO_MAX = None

ZONE_WARNING_TIME = 3
ZONE_ALERT_TIME = 5
DANGER_AREA_COLOR = (0, 0, 255)

# ë‚™ìƒ íŒë‹¨ ì„¤ì •
FALL_CONFIDENCE_THRESHOLD = 0.60
FALL_FRAMES = 3
FALL_TRANSITION_TIME = 1.0

# MQTT RAW ë°ì´í„° ë°œí–‰ ì£¼ê¸° (í”„ë ˆì„)
RAW_PUBLISH_INTERVAL = 15


# ============================================
# MoveNet í¬ì¦ˆ ì¶”ì • (ìˆ˜ì •ëœ ë²„ì „)
# ============================================
class MoveNetPose:
    """ë¼ì¦ˆë² ë¦¬íŒŒì´5ìš© MoveNet Thunder (ì •í™•í•œ ì¢Œí‘œ ë³µì› í¬í•¨)"""

    def __init__(self, model_type='thunder', device='cpu'):
        print(f"Loading MoveNet {model_type}...")
        self.model_type = model_type
        self.input_size = 256 if model_type == 'thunder' else 192
        model_path = f'movenet_{model_type}.tflite'
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"MoveNet TFLite model not found: {model_path}")

        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        print(f"[{now_str()}] âœ… MoveNet loaded: {model_path}")

    def predict(self, frame, bboxes, scores=None):
        if not hasattr(self, 'interpreter') or bboxes is None or len(bboxes) == 0:
            return []

        poses = []
        input_details = self.interpreter.get_input_details()
        output_details = self.interpreter.get_output_details()
        model_dtype = input_details[0]['dtype']

        frame_h, frame_w, _ = frame.shape

        for bbox in bboxes:
            x1_orig, y1_orig, x2_orig, y2_orig = bbox.astype(int)

            # --- [ìˆ˜ì •] âœ… ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ì •ì‚¬ê°í˜• í¬ë¡­ ë¡œì§ ---
            # 1. ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¤‘ì‹¬ê³¼ í¬ê¸° ê³„ì‚°
            center_x = (x1_orig + x2_orig) / 2
            center_y = (y1_orig + y2_orig) / 2
            bbox_w = x2_orig - x1_orig
            bbox_h = y2_orig - y1_orig

            # 2. ì •ì‚¬ê°í˜•ì„ ë§Œë“¤ê¸° ìœ„í•œ ìµœëŒ€ ê¸¸ì´ ì„ íƒ
            square_size = max(bbox_w, bbox_h)

            # 3. ì •ì‚¬ê°í˜• í¬ë¡­ ì¢Œí‘œ ê³„ì‚° (í”„ë ˆì„ ê²½ê³„ í™•ì¸)
            x1_crop = max(0, int(center_x - square_size / 2))
            y1_crop = max(0, int(center_y - square_size / 2))
            x2_crop = min(frame_w, int(center_x + square_size / 2))
            y2_crop = min(frame_h, int(center_y + square_size / 2))

            # 4. ì´ë¯¸ì§€ í¬ë¡­
            crop_img = frame[y1_crop:y2_crop, x1_crop:x2_crop]
            crop_h, crop_w, _ = crop_img.shape
            if crop_h == 0 or crop_w == 0:
                continue

            # --- [ìˆ˜ì •] âœ… ëª¨ë¸ ì…ë ¥ ì „ì²˜ë¦¬ ë° íƒ€ì… í•¸ë“¤ë§ ---
            input_tensor = tf.image.resize(crop_img, [self.input_size, self.input_size])

            if model_dtype == np.float32:
                input_tensor = tf.cast(input_tensor, tf.float32) / 255.0
            else: # uint8, int8
                input_tensor = tf.cast(input_tensor, model_dtype)

            input_tensor = tf.expand_dims(input_tensor, axis=0)

            # --- Inference ---
            self.interpreter.set_tensor(input_details[0]['index'], input_tensor)
            self.interpreter.invoke()
            keypoints_with_scores = np.squeeze(self.interpreter.get_tensor(output_details[0]['index']))

            # --- [ìˆ˜ì •] âœ… ë‹¨ìˆœí•˜ê³  ì •í™•í•œ ì¢Œí‘œ ì—­ë³€í™˜ ---
            # ëª¨ë¸ ì¶œë ¥(0~1) -> í¬ë¡­ ì´ë¯¸ì§€ ì¢Œí‘œ -> ì›ë³¸ í”„ë ˆì„ ì¢Œí‘œ
            kp_y = keypoints_with_scores[:, 0] * crop_h + y1_crop
            kp_x = keypoints_with_scores[:, 1] * crop_w + x1_crop
            kp_score = keypoints_with_scores[:, 2]

            keypoints = np.stack([kp_x, kp_y, kp_score], axis=1)

            proposal_score = float(np.mean(keypoints[:, 2]))
            poses.append({'keypoints': keypoints, 'proposal_score': proposal_score, 'bbox': bbox})

        return poses


# ============================================
# Rule ê¸°ë°˜ ìì„¸ ë¶„ë¥˜
# ============================================

def estimate_motion(prev_kp, curr_kp):
    """í‰ê·  í‚¤í¬ì¸íŠ¸ ì´ë™ëŸ‰"""
    if prev_kp is None or len(prev_kp) == 0:
        return 0.0

    valid = (prev_kp[:, 2] > 0.2) & (curr_kp[:, 2] > 0.2)
    if np.sum(valid) < 5:
        return 0.0

    diffs = np.linalg.norm(curr_kp[valid, :2] - prev_kp[valid, :2], axis=1)
    motion = float(np.mean(diffs))

    return motion


# ============================================
# Rule ê¸°ë°˜ ìì„¸ ë¶„ë¥˜ (ìˆ˜ì •ëœ ë²„ì „)
# ============================================
def detect_fall_rule_based(keypoints, prev_keypoints=None):
    """ëª¸í†µ ê°ë„ë¥¼ í•µì‹¬ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì•ˆì •ì ì¸ ìì„¸ ë¶„ë¥˜ ë¡œì§"""

    conf = float(np.mean(keypoints[:, 2]))
    valid_kp = keypoints[keypoints[:, 2] > 0.2]

    if len(valid_kp) < 5:
        return 'Unknown', conf * 0.3, {}

    # 1. ê¸°ë³¸ íŠ¹ì§•
    width = valid_kp[:, 0].max() - valid_kp[:, 0].min()
    height = valid_kp[:, 1].max() - valid_kp[:, 1].min()
    ratio = width / (height + 1e-6)

    # 2. ì£¼ìš” ê´€ì ˆ ìœ„ì¹˜
    shoulder_y = [keypoints[i][1] for i in [5, 6] if keypoints[i][2] > 0.2]
    shoulder_x = [keypoints[i][0] for i in [5, 6] if keypoints[i][2] > 0.2]
    hip_y = [keypoints[i][1] for i in [11, 12] if keypoints[i][2] > 0.15]
    hip_x = [keypoints[i][0] for i in [11, 12] if keypoints[i][2] > 0.15]
    ankle_y = [keypoints[i][1] for i in [15, 16] if keypoints[i][2] > 0.15]
    
    # 3. í•µì‹¬ íŠ¹ì§• ê³„ì‚°
    ha_dist = abs(np.mean(ankle_y) - np.mean(hip_y)) if len(hip_y) > 0 and len(ankle_y) > 0 else 0
    lower_ratio = ha_dist / (height + 1e-6)
    
    torso_angle = 0
    torso_vertical = False
    if len(shoulder_y) > 0 and len(hip_y) > 0 and len(shoulder_x) > 0 and len(hip_x) > 0:
        torso_height = abs(np.mean(shoulder_y) - np.mean(hip_y))
        torso_width = abs(np.mean(shoulder_x) - np.mean(hip_x))
        torso_angle = np.degrees(np.arctan2(torso_width, torso_height + 1e-6))
        # ëª¸í†µì´ ìˆ˜ì§ì— ê°€ê¹Œìš´ê°€? (ê°ë„ê°€ 35ë„ ë¯¸ë§Œ)
        if torso_angle < 35:
            torso_vertical = True

    motion = estimate_motion(prev_keypoints, keypoints) if prev_keypoints is not None else 0

    details = {
        "ratio": f"{ratio:.2f}", "height": f"{height:.0f}", "lower_r": f"{lower_ratio:.2f}",
        "torso_angle": f"{torso_angle:.1f}", "motion": f"{motion:.1f}"
    }

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸ”´ 1ìˆœìœ„: Lying Down (ëˆ„ì›€/ë‚™ìƒ) - ëª¸í†µ ê°ë„ê°€ í•µì‹¬ ì¦ê±°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì¡°ê±´: ëª¸í†µì´ ê±°ì˜ ìˆ˜í‰(ê°ë„ 60ë„ ì´ìƒ)ì´ê³ , ì „ì²´ ë¹„ìœ¨ë„ ë„“ì–´ì•¼ í•¨
    if torso_angle > 60 and ratio > 1.0:
        return 'Lying Down', conf * 0.98, details
    # ì¡°ê±´: ëª¸í†µì´ ê½¤ ê¸°ìš¸ì—ˆê³ (45ë„ ì´ìƒ), ë†’ì´ê°€ ë§¤ìš° ë‚®ìŒ
    if torso_angle > 45 and height < 120 and ratio > 0.9:
        return 'Lying Down', conf * 0.95, details

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸŸ¡ 2ìˆœìœ„: Sitting (ì•‰ìŒ) - ìˆ˜ì§ ëª¸í†µì´ í•µì‹¬ ì¦ê±°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì¡°ê±´: ëª¸í†µì´ ìˆ˜ì§ì´ê³ , í•˜ì²´ ë¹„ìœ¨ì´ ë‚®ìŒ (ë‹¤ë¦¬ê°€ ì ‘í˜€ìˆìŒ)
    if torso_vertical and lower_ratio < 0.38:
        return 'Sitting', conf * 0.95, details
    # ì¡°ê±´: ëª¸í†µì´ ìˆ˜ì§ì´ê³ , ì „ì²´ í‚¤ê°€ ì‘ìŒ
    if torso_vertical and height < 150 and ratio > 0.5:
        return 'Sitting', conf * 0.90, details

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸŸ¢ 3ìˆœìœ„: Standing / Walking (ì„œìˆìŒ/ê±¸ìŒ)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì¡°ê±´: ëª¸í†µì´ ìˆ˜ì§ì´ê³ , í•˜ì²´ ë¹„ìœ¨ì´ ë†’ìŒ (ë‹¤ë¦¬ê°€ í´ì ¸ìˆìŒ)
    if torso_vertical and lower_ratio >= 0.38:
        if motion > 6:
            return 'Walking', conf * 0.95, details
        else:
            return 'Standing', conf * 0.95, details
    
    # ìœ„ ëª¨ë“  ëª…í™•í•œ ê·œì¹™ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ 'Unknown'
    return 'Unknown', conf * 0.5, details


# ============================================
# ìœ„í—˜êµ¬ì—­ í•¨ìˆ˜
# ============================================

def get_location_details(bbox):
    """ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì´ìš©í•´ ì¤‘ì‹¬ x, í•˜ë‹¨ y ì¢Œí‘œë¥¼ ë°˜í™˜"""
    x1, y1, x2, y2 = bbox
    center_x = (x1 + x2) / 2
    bottom_y = y2
    return int(center_x), int(bottom_y)


def is_in_danger_zone(bbox, frame_width, frame_height):
    """ë°”ìš´ë”©ë°•ìŠ¤ê°€ ìœ„í—˜êµ¬ì—­ì— ìˆëŠ”ì§€ í™•ì¸"""
    center_x, bottom_y = get_location_details(bbox)

    if USE_RATIO:
        x_min = int(frame_width * DANGER_X_RATIO_MIN) if DANGER_X_RATIO_MIN is not None else 0
        x_max = int(frame_width * DANGER_X_RATIO_MAX) if DANGER_X_RATIO_MAX is not None else frame_width
        y_min = int(frame_height * DANGER_Y_RATIO_MIN) if DANGER_Y_RATIO_MIN is not None else 0
        y_max = int(frame_height * DANGER_Y_RATIO_MAX) if DANGER_Y_RATIO_MAX is not None else frame_height
    else:
        x_min = DANGER_X_MIN if DANGER_X_MIN is not None else 0
        x_max = DANGER_X_MAX if DANGER_X_MAX is not None else frame_width
        y_min = DANGER_Y_MIN if DANGER_Y_MIN is not None else 0
        y_max = DANGER_Y_MAX if DANGER_Y_MAX is not None else frame_height

    in_danger_x = True
    if x_min is not None and center_x < x_min:
        in_danger_x = False
    if x_max is not None and center_x > x_max:
        in_danger_x = False

    in_danger_y = True
    if y_min is not None and bottom_y < y_min:
        in_danger_y = False
    if y_max is not None and bottom_y > y_max:
        in_danger_y = False

    return in_danger_x and in_danger_y


def draw_danger_area(frame, show_gui):
    """ìœ„í—˜êµ¬ì—­ ì‹œê°í™” (GUI ëª¨ë“œì¼ ë•Œë§Œ)"""
    if not show_gui:
        return frame

    h, w = frame.shape[:2]
    overlay = frame.copy()

    if USE_RATIO:
        x_min = int(w * DANGER_X_RATIO_MIN) if DANGER_X_RATIO_MIN is not None else 0
        x_max = int(w * DANGER_X_RATIO_MAX) if DANGER_X_RATIO_MAX is not None else w
        y_min = int(h * DANGER_Y_RATIO_MIN) if DANGER_Y_RATIO_MIN is not None else 0
        y_max = int(h * DANGER_Y_RATIO_MAX) if DANGER_Y_RATIO_MAX is not None else h
    else:
        x_min = DANGER_X_MIN if DANGER_X_MIN is not None else 0
        x_max = DANGER_X_MAX if DANGER_X_MAX is not None else w
        y_min = DANGER_Y_MIN if DANGER_Y_MIN is not None else 0
        y_max = DANGER_Y_MAX if DANGER_Y_MAX is not None else h

    cv2.rectangle(overlay, (x_min, y_min), (x_max, y_max), DANGER_AREA_COLOR, -1)
    cv2.addWeighted(overlay, 0.2, frame, 0.8, 0, frame)
    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), DANGER_AREA_COLOR, 3)
    cv2.putText(frame, "DANGER ZONE", (x_min + 10, y_min + 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, DANGER_AREA_COLOR, 2)

    return frame


def draw_zone_warnings(frame, zone_warnings, show_gui):
    """ì—¬ëŸ¬ ìœ„í—˜êµ¬ì—­ ê²½ê³ ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ (GUI ëª¨ë“œì¼ ë•Œë§Œ)"""
    if not show_gui:
        return frame

    y_offset = 50

    for i, (track_id, elapsed_time, status) in enumerate(zone_warnings):
        if status == 'warning':
            color = (0, 255, 255)
            text = f"WARNING! Worker #{track_id} in danger zone {elapsed_time:.1f}s"
        elif status == 'danger':
            color = (0, 0, 255)
            text = f"DANGER! Worker #{track_id} in danger zone {elapsed_time:.1f}s"
        else:
            continue

        y_pos = y_offset + (i * 35)
        cv2.rectangle(frame, (10, y_pos), (650, y_pos + 30), color, -1)
        cv2.putText(frame, text, (15, y_pos + 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    return frame

# ============================================
# Improved Tracker
# ============================================
class ImprovedTracker:
    """IoU ê¸°ë°˜ íŠ¸ë˜ì»¤ (í•„í„° ì™„í™” ë²„ì „)"""

    def __init__(self, max_age=12):
        self.tracks = {}
        self.next_id = 1
        self.max_age = max_age
        self.frame_count = 0

    def _is_valid_person(self, keypoints, bbox):
        avg_conf = np.mean(keypoints[:, 2])
        if avg_conf < 0.15:
            return False
        important = [5, 6, 11, 12]
        imp_conf = [keypoints[i][2] for i in important if i < len(keypoints)]
        if len(imp_conf) < 2 or np.mean(imp_conf) < 0.25:
            return False
        x1, y1, x2, y2 = bbox
        w, h = x2 - x1, y2 - y1
        if w < 20 or h < 30 or h / (w + 1e-9) < 0.15:
            return False
        return True

    def _iou(self, b1, b2):
        x1, y1 = max(b1[0], b2[0]), max(b1[1], b2[1])
        x2, y2 = min(b1[2], b2[2]), min(b1[3], b2[3])
        if x2 < x1 or y2 < y1: return 0
        inter = (x2 - x1) * (y2 - y1)
        area1 = (b1[2]-b1[0])*(b1[3]-b1[1])
        area2 = (b2[2]-b2[0])*(b2[3]-b2[1])
        return inter / (area1 + area2 - inter + 1e-9)

    def update(self, detections):
        self.frame_count += 1
        for tid in list(self.tracks.keys()):
            self.tracks[tid]['age'] += 1
            if self.tracks[tid]['age'] > self.max_age:
                del self.tracks[tid]

        valid = [d for d in detections if self._is_valid_person(d['keypoints'], d['bbox'])]
        for det in valid:
            best_iou, best_id = 0, None
            for tid, tr in self.tracks.items():
                iou = self._iou(det['bbox'], tr['bbox'])
                if iou > best_iou and iou > 0.3:
                    best_iou, best_id = iou, tid
            if best_id:
                self.tracks[best_id]['bbox'] = det['bbox']
                self.tracks[best_id]['keypoints'].append(det['keypoints'])
                self.tracks[best_id]['age'] = 0
            else:
                self.tracks[self.next_id] = {'bbox': det['bbox'], 'keypoints': [det['keypoints']], 'age': 0}
                self.next_id += 1
        return list(self.tracks.keys())

# ============================================
# YOLOv8 ê²€ì¶œê¸° (ì•ˆì •í™” ë²„ì „)
# ============================================
class ImprovedYOLODetector:
    """YOLOv8n ì‚¬ëŒ ê²€ì¶œ - í˜¸í™˜ì„± ë° ì•ˆì •ì„± ê°œì„ """

    def __init__(self, model_name='yolov8n.pt', conf_thres=0.45, device='cpu'): # conf_thres ì‚´ì§ ë‚®ì¶¤
        self.model = YOLO(model_name)
        self.conf_thres = conf_thres
        self.device = device

    def detect(self, frame):
        if frame.shape[-1] == 1:
            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        elif frame.shape[-1] == 4:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)

        results = self.model.predict(
            source=frame, conf=self.conf_thres, imgsz=416,
            classes=[0], device=self.device, verbose=False
        )

        if len(results) == 0 or len(results[0].boxes) == 0:
            return np.array([]), np.array([])

        boxes = results[0].boxes
        xyxy = boxes.xyxy.cpu().numpy()
        confs = boxes.conf.cpu().numpy()
        
        bboxes, scores = [], []
        for (x1, y1, x2, y2), conf in zip(xyxy, confs):
            w, h = x2 - x1, y2 - y1
            
            # [ìˆ˜ì •] âœ… í•„í„°ë§ ì¡°ê±´ ëŒ€í­ ì™„í™”
            # ìµœì†Œ í¬ê¸° ì¡°ê±´ë§Œ ë‚¨ê¹€
            if w < 20 or h < 20:
                continue
            
            bboxes.append([x1, y1, x2, y2])
            scores.append(conf)

        if DEBUG_MODE:
            # YOLO íƒì§€ ê²°ê³¼ê°€ 10í”„ë ˆì„ì— í•œë²ˆì”©ë§Œ ë³´ì´ë„ë¡ ìˆ˜ì • (í„°ë¯¸ë„ ê¹”ë”í•˜ê²Œ)
            if hasattr(self, 'frame_counter'):
                self.frame_counter += 1
            else:
                self.frame_counter = 1
            
            if self.frame_counter % 10 == 0:
                print(f"[YOLO] Detected {len(bboxes)} persons")

        return np.array(bboxes, dtype=np.float32), np.array(scores, dtype=np.float32)


# ============================================
# MQTT ë°œí–‰ í•¨ìˆ˜
# ============================================
def on_connect(client, userdata, flags, rc):
    """MQTT ì—°ê²° ì½œë°±"""
    if rc == 0:
        print(f"[{now_str()}] âœ… MQTT Connected successfully.")
    else:
        print(f"[{now_str()}] âŒ MQTT Connection failed with code {rc}")

def publish_mqtt_message(client, topic, payload):
    """JSON ë©”ì‹œì§€ë¥¼ MQTTë¡œ ë°œí–‰"""
    try:
        json_payload = json.dumps(payload, ensure_ascii=False)
        client.publish(topic, json_payload, qos=0)
        if DEBUG_MODE:
            msg_type = "ALERT" if "ALERT" in topic else "RAW"
            level = payload.get("level", "N/A")
            print(f"[{now_str()}] [MQTT SEND - {msg_type}:{level}] {topic}")
    except Exception as e:
        print(f"[{now_str()}] [MQTT ERROR] Failed to publish to {topic}: {e}")


# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================

def main():
    parser = argparse.ArgumentParser(description='Improved Fall Detection with MQTT Client')
    parser.add_argument('--camera', type=str, default='0', help='Camera source')
    parser.add_argument('--device', type=str, default='cpu', help='cpu only for RPi5')
    parser.add_argument('--model', type=str, default='thunder', choices=['thunder', 'lightning'])
    parser.add_argument('--show-gui', action='store_true', help='Show GUI window with visualizations')
    args = parser.parse_args()

    print("="*60)
    print("Improved Fall Detection System (MQTT Client)")
    print("- Object Detection: YOLOv8n (Enhanced Filtering)")
    print("- Pose: MoveNet " + args.model.title())
    print("- Device: CPU")
    print(f"- MQTT Broker: {MQTT_BROKER}:{MQTT_PORT} / Module: {FALL_MODULE}")
    print(f"- GUI Mode: {'Enabled' if args.show_gui else 'Disabled'}")
    print("="*60)

    # ëª¨ë¸ ë¡œë“œ
    print("\n1ï¸âƒ£ Loading models...")
    detector = ImprovedYOLODetector(model_name='yolov8n.pt', conf_thres=0.5, device='cpu')
    pose_model = MoveNetPose(model_type=args.model)

    # íŠ¸ë˜ì»¤
    tracker = ImprovedTracker(max_age=12)

    # MQTT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    mqtt_client = mqtt.Client(client_id="PE_Client", protocol=mqtt.MQTTv311)

    # ì‚¬ìš©ì ì´ë¦„ ë° ë¹„ë°€ë²ˆí˜¸ ì„¤ì •
    mqtt_client.username_pw_set(MQTT_USERNAME, MQTT_PASSWORD)

    mqtt_client.on_connect = on_connect
    try:
        mqtt_client.connect(MQTT_BROKER, MQTT_PORT, 60)
        mqtt_client.loop_start()
    except Exception as e:
        print(f"[{now_str()}] âŒ Failed to connect to MQTT broker: {e}")

    # ì¹´ë©”ë¼
    print("\n2ï¸âƒ£ Opening camera...")
    cam_source = args.camera
    if cam_source.isdigit():
        cap = cv2.VideoCapture(int(cam_source))
    else:
        cap = cv2.VideoCapture(cam_source)

    if not cap.isOpened():
        print(f"[{now_str()}] âŒ Cannot open camera!")
        mqtt_client.loop_stop()
        return

    print(f"[{now_str()}] âœ… Camera opened")

    # ìƒíƒœ ë³€ìˆ˜
    fall_counters = {}
    fall_alerted = {}
    zone_timers = {}
    previous_states = {}
    alert_sent_zone = {}

    fps_time = time.time()
    frame_count = 0

    # [ìˆ˜ì •] âœ… YOLO í”„ë ˆì„ ìŠ¤í‚µ ë° ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜
    yolo_skip_counter = 0
    YOLO_SKIP_FRAMES = 3
    last_tracked_data = {} # ì¶”ì  ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜

    quit_msg = "Press 'q' to quit" if args.show_gui else "Press Ctrl+C to quit"
    print(f"[{now_str()}] 3ï¸âƒ£ Starting detection... ({quit_msg})\n")

    while True:
        try:
            ret, frame = cap.read()
            if not ret:
                print(f"[{now_str()}] End of stream or video.")
                break

            frame_count += 1
            h, w = frame.shape[:2]
            current_time = time.time()
            
            # ìœ„í—˜êµ¬ì—­ ê·¸ë¦¬ê¸° (ë§¤ í”„ë ˆì„)
            frame = draw_danger_area(frame, args.show_gui)

            # [ìˆ˜ì •] âœ… í”„ë ˆì„ ìŠ¤í‚µ ë¡œì§
            yolo_skip_counter += 1
            if yolo_skip_counter >= YOLO_SKIP_FRAMES:
                yolo_skip_counter = 0 # ì¹´ìš´í„° ë¦¬ì…‹
                last_tracked_data = {} # ì´ì „ ì¶”ì  ë°ì´í„° ì´ˆê¸°í™”

                # --- ì´ ë¸”ë¡ ì•ˆì—ì„œë§Œ íƒì§€/ì¶”ì  íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰ ---
                bboxes, scores = detector.detect(frame)
                
                detections = []
                if len(bboxes) > 0:
                    poses = pose_model.predict(frame, bboxes, scores)
                    for pose, bbox in zip(poses, bboxes):
                        detections.append({
                            'bbox': bbox,
                            'keypoints': pose['keypoints'],
                            'score': pose['proposal_score']
                        })
                
                current_tracks_ids = tracker.update(detections)
                
                # --- ì¶”ì ëœ ê°ì²´ë“¤ì˜ ìµœì¢… ì •ë³´ë¥¼ ì €ì¥ ---
                for track_id in current_tracks_ids:
                    if track_id in tracker.tracks:
                        track = tracker.tracks[track_id]
                        if len(track['keypoints']) > 0:
                            last_tracked_data[track_id] = {
                                'bbox': track['bbox'],
                                'keypoints_list': track['keypoints'] # ìƒíƒœ ë¶„ì„ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì €ì¥
                            }

            # --- [ìˆ˜ì •] âœ… ë§¤ í”„ë ˆì„ ìƒíƒœ ë¶„ì„ ë° ì‹œê°í™”ëŠ” ì €ì¥ëœ 'last_tracked_data' ì‚¬ìš© ---
            current_tracks = list(last_tracked_data.keys())
            raw_detections_list = []
            is_person_detected = len(current_tracks) > 0

            # ê° íŠ¸ë™ ì²˜ë¦¬
            for track_id in current_tracks:
                tracked_info = last_tracked_data[track_id]
                bbox = tracked_info['bbox']
                keypoints_list = tracked_info['keypoints_list']
                
                if len(keypoints_list) < 1:
                    continue

                current_kp = keypoints_list[-1]
                prev_kp = keypoints_list[-2] if len(keypoints_list) > 1 else None
                
                x1, y1, x2, y2 = bbox.astype(int)
                center_x, bottom_y = get_location_details(bbox)

                # --- [A] ìœ„í—˜êµ¬ì—­ ì²´í¬ (ZONE) ---
                in_zone = is_in_danger_zone(bbox, w, h)
                
                if in_zone:
                    if track_id not in zone_timers:
                        zone_timers[track_id] = current_time
                        alert_sent_zone[track_id] = False
                    
                    elapsed = current_time - zone_timers[track_id]
                    
                    if elapsed >= ZONE_ALERT_TIME and not alert_sent_zone.get(track_id, False):
                        alert_payload = {
                            "timestamp": now_str(), "module": FALL_MODULE, "level": "CRITICAL",
                            "message": f"ğŸš¨ DANGER ZONE CRITICAL: Worker #{track_id} in high-risk area for {elapsed:.1f}s.",
                            "details": [{"track_id": track_id, "action": "InDangerZoneCritical", "location": f"({center_x}, {bottom_y})"}]
                        }
                        publish_mqtt_message(mqtt_client, ALERT_TOPIC, alert_payload)
                        alert_sent_zone[track_id] = True
                else:
                    if track_id in zone_timers:
                        del zone_timers[track_id]
                        alert_sent_zone[track_id] = False
                
                # --- [B] ë‚™ìƒ ê°ì§€ (FALL) ---
                rule_action, rule_conf, details = detect_fall_rule_based(current_kp, prev_kp)
                
                # ğŸ”´ ìƒíƒœ ì „ì´ ë¶„ì„
                prev_action = previous_states.get(track_id, {}).get('action', None)
                state_start_time = previous_states.get(track_id, {}).get('state_start_time', current_time)
                
                final_action = rule_action
                new_state_start_time = state_start_time

                if rule_action == 'Lying Down':
                    final_action = 'Fall Down'
                    if prev_action != 'Fall Down':
                        new_state_start_time = current_time
                elif prev_action == 'Fall Down' and rule_action in ['Standing', 'Walking', 'Sitting']:
                    new_state_start_time = current_time
                elif prev_action != rule_action:
                    new_state_start_time = current_time
                
                previous_states[track_id] = {
                    'action': final_action,
                    'state_start_time': new_state_start_time
                }
                
                # ë‚™ìƒ ì¹´ìš´í„°
                if final_action == 'Fall Down' and rule_conf >= FALL_CONFIDENCE_THRESHOLD:
                    fall_counters[track_id] = fall_counters.get(track_id, 0) + 1
                else:
                    fall_counters[track_id] = 0
                    fall_alerted[track_id] = False

                # ğŸš¨ FALL CRITICAL ALERT (ë‚™ìƒ í™•ì •)
                if fall_counters.get(track_id, 0) >= FALL_FRAMES and not fall_alerted.get(track_id, False):
                    alert_payload = {
                        "timestamp": now_str(), "module": FALL_MODULE, "level": "CRITICAL",
                        "message": f"ğŸš¨ CRITICAL FALL DETECTED: Worker #{track_id} is {final_action.lower()}.",
                        "details": [{"track_id": track_id, "action": final_action, "confidence": float(rule_conf), "location": f"({center_x}, {bottom_y})"}]
                    }
                    publish_mqtt_message(mqtt_client, ALERT_TOPIC, alert_payload)
                    fall_alerted[track_id] = True
                    print(f"[{now_str()}] ğŸš¨ğŸš¨ [CRITICAL ALERT SENT] Fall alert for Worker #{track_id}")

                # RAW ë°ì´í„° ê¸°ë¡
                raw_detections_list.append({
                    "track_id": int(track_id), "object_type": "Person", "action": final_action,
                    "confidence": float(rule_conf), "x_center": center_x, "y_bottom": bottom_y,
                    "in_danger_zone": in_zone
                })

            # GUI ëª¨ë“œ: í™”ë©´ì— ì‹œê°í™”
            if args.show_gui:
                for track_id in current_tracks:
                    tracked_info = last_tracked_data[track_id]
                    bbox = tracked_info['bbox']
                    current_kp = tracked_info['keypoints_list'][-1]
                    
                    x1, y1, x2, y2 = bbox.astype(int)
                    
                    final_action = previous_states.get(track_id, {}).get('action', 'Unknown')
                    
                    if fall_counters.get(track_id, 0) >= FALL_FRAMES: clr = (0, 0, 255)
                    elif final_action == 'Fall Down': clr = (0, 0, 255)
                    elif final_action == 'Standing': clr = (0, 255, 0)
                    elif final_action == 'Sitting': clr = (0, 255, 255)
                    elif final_action == 'Walking': clr = (255, 255, 0)
                    else: clr = (255, 255, 255)
                    
                    cv2.rectangle(frame, (x1, y1), (x2, y2), clr, 2)
                    cv2.putText(frame, f'ID:{track_id} {final_action}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, clr, 2)
                    
                    # [ì¶”ê°€] âœ… í‚¤í¬ì¸íŠ¸ ì‹œê°í™”
                    for i in range(len(current_kp)):
                        if current_kp[i, 2] > 0.3:
                            kx, ky = int(current_kp[i, 0]), int(current_kp[i, 1])
                            cv2.circle(frame, (kx, ky), 3, (255, 0, 255), -1)

                fps = 1.0 / (time.time() - fps_time + 1e-6)
                info_text = f'FPS: {fps:.1f} | Persons: {len(current_tracks)}'
                cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                cv2.imshow('Fall Detection System', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            # 5. ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì „ì†¡
            try:
                ret_enc, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 50])
                if ret_enc:
                    jpg_as_text = base64.b64encode(buffer.tobytes())
                    mqtt_client.publish(FALL_VIDEO_TOPIC, jpg_as_text, qos=0)
            except Exception as e:
                print(f"[{now_str()}] [ERROR] âŒ FALL Video streaming publish failed: {e}")

            # 6. RAW ë°ì´í„° ë°œí–‰ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if frame_count % RAW_PUBLISH_INTERVAL == 0 and is_person_detected:
                raw_payload = {
                    "timestamp": now_str(),
                    "module": FALL_MODULE,
                    "level": "INFO",
                    "detections": raw_detections_list,
                    "person_detected": is_person_detected
                }
                mqtt_client.publish(RAW_TOPIC, json.dumps(raw_payload, ensure_ascii=False), qos=0)
            
            fps_time = time.time()
            time.sleep(0.01)

        except KeyboardInterrupt:
            print(f"\n[{now_str()}] [INFO System] Program stopped by user (Ctrl+C).")
            break
        except Exception as e:
            print(f"\n[{now_str()}] [ERROR System] An unexpected error occurred: {e}")
            import traceback
            traceback.print_exc()
            break
    
    # ì •ë¦¬
    cap.release()
    cv2.destroyAllWindows()
    mqtt_client.loop_stop()
    mqtt_client.disconnect()
    print("\n" + "="*60 + "\nProgram terminated.\n" + "="*60)


if __name__ == '__main__':
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    main()